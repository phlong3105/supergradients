{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This tutorial shows how to export SG models to ONNX format for deployment to ONNX-compatible runtimes and accelerators.\n",
    "\n",
    "\n",
    "From this tutorial you will learn:\n",
    "\n",
    "* How to export Semantic Segmentation model to ONNX and do inference with ONNXRuntime / TensorRT\n",
    "* How to enable FP16 / INT8 quantization and export a model with calibration\n",
    "* How to customize confidence threshold parameters for binary segmentation models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## New Export API\n",
    "\n",
    "A new export API for Object Detection introduced in SG 3.2.0 and since SG 3.7.0 we support `model.export()` for our segmentation models.\n",
    "It is aimed to simplify the export process and allow end-to-end export of SG models to ONNX format with a single line of code.\n",
    "\n",
    "### Currently supported models\n",
    "\n",
    "- All Segformer variants\n",
    "- All PPLite variants\n",
    "- All STDC variants\n",
    "- All DDRNet variants\n",
    "\n",
    "### Supported features\n",
    "\n",
    "- Exporting a model with preprocessing (e.g. normalizing/standardizing image according to normalization parameters during training)\n",
    "- Exporting a model with postprocessing (argmax for multiclass, sigmoid + thresholding for binary) - you obtain the ready-to-use semantic masks\n",
    "- FP16 / INT8 quantization support with calibration\n",
    "- Pre- and post-processing steps can be customized by the user if needed\n",
    "- Customising input image shape and batch size\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ykoGiEtLG7bV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "!pip install -qq super_gradients==3.7.1"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Je-_9KmG7bV",
    "outputId": "45c8a021-156e-4c1b-e4ad-db5b4688212c",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-23T15:07:37.409508300Z",
     "start_time": "2024-02-23T15:07:25.575846700Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Minimalistic export example\n",
    "\n",
    "Let start with the most simple example of exporting a model to ONNX format.\n",
    "We will use YoloNAS-S model in this example. All models that suports new export API now expose a `export()` method that can be used to export a model. There is one mandatory argument that should be passed to the `export()` method - the path to the output file. Currently, only `.onnx` format is supported, but we may add support for CoreML and other formats in the future."
   ],
   "metadata": {
    "collapsed": false,
    "id": "h0EWfAetG7bW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "\n",
    "model = models.get(Models.SEGFORMER_B0, pretrained_weights=\"cityscapes\")\n",
    "\n",
    "export_result = model.export(\"segformer.onnx\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVSVkf8oG7bW",
    "outputId": "cc3d33e0-1c11-4b6f-ff95-e2aa5ed594a4",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-23T15:07:55.854605200Z",
     "start_time": "2024-02-23T15:07:37.409508300Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "A lot of work just happened under the hood:\n",
    "\n",
    "* A model was exported to ONNX format using default batch size of 1 and input image shape that was used during training\n",
    "* A preprocessing and postprocessing steps were attached to ONNX graph (`preprocessing=True` and `postprocessing=True` are default values for these parameters)\n",
    "* For pre-processing step, the normalization parameters were extracted from the model itself (to be consistent with the image normalization and channel order used during training)\n",
    "* A post-processing step was attached to the graph. In case of semantic segmentation models, it includes `argmax` operation to obtain the final segmentation mask.\n",
    "* ONNX graph was checked and simplified to improve compatibility with ONNX runtimes."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ILppAMFZG7bW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A return value of `export()` method is an instance of `SegmentationModelExportResult` class.\n",
    "First of all it serves the purpose of storing all the information about the exported model in a single place.\n",
    "It also provides a convenient way to get an example of running the model and getting the output:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YXl28XcWG7bX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "export_result"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JK-VsBHG7bX",
    "outputId": "153dff12-71c7-4086-9001-516b18cb498f",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-23T15:07:55.886494100Z",
     "start_time": "2024-02-23T15:07:55.856120900Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it. You can now use the exported model with any ONNX-compatible runtime or accelerator.\n",
    "To show how it works, we will use ONNX Runtime to run the model and visualize the output. \n",
    "A helper function `run_inference_with_onnx_model` is provided for this purpose. It loads the model using ONNX Runtime and runs it with a sample image."
   ],
   "metadata": {
    "collapsed": false,
    "id": "_djOsc4FG7bX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import matplotlib.pyplot as plt\n",
    "from super_gradients.training.utils.media.image import load_image\n",
    "from super_gradients.training.utils.visualization.segmentation import overlay_segmentation\n",
    "\n",
    "\n",
    "def run_inference_with_onnx_model(export_result, image: np.ndarray, title):\n",
    "    image = cv2.resize(image, (export_result.input_image_shape[1], export_result.input_image_shape[0]))\n",
    "    image_bchw = np.transpose(np.expand_dims(image, 0), (0, 3, 1, 2))\n",
    "\n",
    "    session = onnxruntime.InferenceSession(export_result.output, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "    inputs = [o.name for o in session.get_inputs()]\n",
    "    outputs = [o.name for o in session.get_outputs()]\n",
    "    predictions = session.run(outputs, {inputs[0]: image_bchw})\n",
    "    \n",
    "    [segmentation_mask] = predictions\n",
    "    \n",
    "    overlay = overlay_segmentation(\n",
    "        pred_mask=segmentation_mask[0], image=image, num_classes=19, alpha=0.5\n",
    "    )\n",
    "    f = plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T15:07:55.904548700Z",
     "start_time": "2024-02-23T15:07:55.880498800Z"
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "image = load_image(\"https://deci-pretrained-models.s3.amazonaws.com/sample_images/beatles-abbeyroad.jpg\")\n",
    "run_inference_with_onnx_model(export_result, image, title='Segformer B0')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIAKv7tJG7bY",
    "outputId": "0b1e4b54-65b3-4e9f-d258-744d5d8c687b",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-23T15:08:01.440525500Z",
     "start_time": "2024-02-23T15:07:55.887494800Z"
    }
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Please note that we are using the smallest model variant in this example. \n",
    "It is not very accurate and the result is not perfect which is fine - the goal of this notebook is to demonstrate the concepts.\n",
    "You can use larger models for better results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the snippet above we resize the input image to the shape that was used during training - `(1024, 2048)` and run the model using ONNX Runtime. It is very easy to change the image resolution to your needs. All you need is to specify `input_image_shape` parameter when calling `export()` method:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "export_result = model.export(\"segformer_640_640.onnx\", input_image_shape=(640,640))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkyHxBIpG7bY",
    "outputId": "b90d205f-caf7-4660-a147-dd5e2c210db8",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-23T15:08:05.408174Z",
     "start_time": "2024-02-23T15:08:01.415338300Z"
    }
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we now run the model with the same image, we will see that the output resolution is different:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "run_inference_with_onnx_model(export_result, image, title='Segformer B0 at 640x640')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T15:08:06.243194800Z",
     "start_time": "2024-02-23T15:08:05.389512900Z"
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export of quantized model\n",
    "\n",
    "You can export a model with quantization to FP16 or INT8. To do so, you need to specify the `quantization_mode` argument of `export()` method.\n",
    "\n",
    "Important notes:\n",
    "* Quantization to FP16 requires CUDA / MPS device available and would not work on CPU-only machines.\n",
    "\n",
    "Let's see how it works:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6tU82QMZG7bZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pytorch-quantization==2.1.2 --extra-index-url https://pypi.ngc.nvidia.com"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZiyX1KHSAAF",
    "outputId": "4ecd45ae-ecc0-488b-f674-9073f91cfbe3",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-23T15:08:11.734980500Z",
     "start_time": "2024-02-23T15:08:06.242209300Z"
    }
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from super_gradients.conversion.conversion_enums import ExportQuantizationMode\n",
    "\n",
    "export_result = model.export(\n",
    "    \"segformer_b0_int8.onnx\",\n",
    "    input_image_shape=(640, 640),\n",
    "    quantization_mode=ExportQuantizationMode.INT8 # or ExportQuantizationMode.FP16\n",
    ")\n",
    "\n",
    "run_inference_with_onnx_model(export_result, image, title='Segformer B0 INT-8 Quantized')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "sa65jEIjG7bZ",
    "outputId": "f7b51a56-5a71-497e-88eb-554eba65eaa2",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-23T15:08:22.979984200Z",
     "start_time": "2024-02-23T15:08:11.735487Z"
    }
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Advanced INT-8 quantization options\n",
    "\n",
    "When quantizing a model using `quantization_mode==ExportQuantizationMode.INT8` you can pass a DataLoader to export() function to collect correct statistics of activations to prodice a more accurate quantized model.\n",
    "We expect the DataLoader to return either a tuple of tensors or a single tensor. In case a tuple of tensors is returned by data-loader the first element will be used as input image.\n",
    "You can use existing data-loaders from SG here as is.\n",
    "\n",
    "**Important notes**\n",
    "* A `calibration_loader` should use same image normalization parameters that were used during training.\n",
    "\n",
    "In the example below we use a dummy data-loader for sake of showing how to use this feature. You should use your own data-loader here."
   ],
   "metadata": {
    "collapsed": false,
    "id": "NYr7b65NG7ba",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from super_gradients.conversion import ExportQuantizationMode\n",
    "\n",
    "# THIS IS ONLY AN EXAMPLE. YOU SHOULD USE YOUR OWN DATA-LOADER HERE\n",
    "dummy_calibration_dataset = [torch.randn((3, 640, 640), dtype=torch.float32) for _ in range(32)]\n",
    "dummy_calibration_loader = DataLoader(dummy_calibration_dataset, batch_size=8, num_workers=0)\n",
    "# THIS IS ONLY AN EXAMPLE. YOU SHOULD USE YOUR OWN DATA-LOADER HERE\n",
    "\n",
    "export_result = model.export(\n",
    "    \"segformer_b0_int8_calibrated.onnx\",\n",
    "    input_image_shape=(640, 640),\n",
    "    quantization_mode=ExportQuantizationMode.INT8 # or ExportQuantizationMode.FP16\n",
    ")\n",
    "\n",
    "run_inference_with_onnx_model(export_result, image, title='Segformer B0 INT-8 Quantized With Calibration')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W-wjSejMG7ba",
    "outputId": "21dd28f1-80c7-4c1a-9bd8-59b3fa254931",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-23T15:08:33.352353300Z",
     "start_time": "2024-02-23T15:08:22.915234200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Limitations\n",
    "\n",
    "* Dynamic batch size / input image shape is not supported yet. You can only export a model with a fixed batch size and input image shape.\n",
    "* TensorRT of version 8.4.1 or higher is required.\n",
    "* Quantization to FP16 requires CUDA / MPS device available."
   ],
   "metadata": {
    "collapsed": false,
    "id": "KZK3oB3EG7ba",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "n5Su7rpSG7ba",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
