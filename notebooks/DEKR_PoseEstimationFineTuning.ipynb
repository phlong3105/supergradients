{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pose Estimation Fine Tuninig with Super Gradients\n",
    "\n",
    "In this tutorial notebook we demonstrate how to fine tune a pose estimation model using SuperGradients. It is recommended that you go over [Pose Estimation tutorial](https://docs.deci.ai/super-gradients/documentation/source/PoseEstimation.html) docs first to get familiar with terminology and concepts we use here.\n",
    "\n",
    "From this tutorial you will learn:\n",
    "* How to implement a custom dataset class for pose estimation task\n",
    "* How to instantiate a pre-trained pose estimation model and change number of joints it predicts to fit your dataset\n",
    "* How to fine-tune a pose estimation model using SuperGradients\n",
    "* How to visualize training progress and results in Tensorboard\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "obAU90uggSAd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q super_gradients==3.7.1 scikit-learn"
   ],
   "metadata": {
    "id": "3UZJqTehg0On",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6646bf4f-7ecb-4fb1-90fe-a79b62e32bc2"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "The first thing we need is our dataset. For this tutorial we will be using [Animals Pose](https://sites.google.com/view/animal-pose/) dataset. It is a relatively small dataset of 6K+ instances of animals, each annotated with 20 keypoints.\n",
    "\n",
    "![](https://lh6.googleusercontent.com/hehW9yRzdcniQ2i1Ts65ceGERa70cBbaLlRixxu7HlUMHabt8HdgcxutG4vmVOas-U1h6g=w16383)\n",
    "\n",
    "### Download dataset from here\n",
    "\n",
    "https://drive.google.com/drive/folders/1xxm6ZjfsDSmv6C9JvbgiGrmHktrUjV5x?usp=sharing\n",
    "\n",
    "You need to download the Animal Pose Dataset from Google Drive and update `ANIMALS_POSE_DATA_DIR` to point to this location:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "C2VGCa6ogSAe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/file/d/1BWZ2aMl2CVgDwRJ214TPvZey_F9GMfTq/view?usp=share_link --fuzzy -q\n",
    "!gdown https://drive.google.com/file/d/1AZEEYwAe41_FYtSh0G61oIutskMdRFUk/view?usp=share_link --fuzzy -q\n",
    "!unzip -q -o \"images.zip\"\n",
    "!ls -l /content"
   ],
   "metadata": {
    "id": "7n3sEn31gXHD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a1174212-6606-4023-8d99-9e9a01c3687d"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "ANIMALS_POSE_DATA_DIR = \"/content\""
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:10.332075300Z",
     "start_time": "2023-06-05T11:08:10.178190700Z"
    },
    "id": "9CHPNavBgSAf"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset preparation\n",
    "\n",
    "Animals Pose dataset uses COCO-style annotation format.\n",
    "Unfortunately it's not 100% compatible with COCO parser from `pycocotools`.\n",
    "So we have to write out own parser. That is not a big issue, since the format is very simple.\n",
    "There are 3 main parts of the annotation file: \"images\", \"annotations\" and \"categories\".\n",
    "\n",
    "Here is an example of the annotation file:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"images\": {\n",
    "        \"1\": \"2007_000063.jpg\",\n",
    "        \"2\": \"2007_000175.jpg\",\n",
    "        \"6\": \"2007_000491.jpg\",\n",
    "        ...\n",
    "        \"4606\": \"sh97.jpg\",\n",
    "        \"4607\": \"sh98.jpeg\",\n",
    "        \"4608\": \"sh99.jpeg\"\n",
    "    },\n",
    "    \"annotations\": [\n",
    "        {\n",
    "            \"image_id\": 1,\n",
    "            \"bbox\": [123, 115, 379, 275],\n",
    "            \"keypoints\": [\n",
    "                [193, 216, 1],\n",
    "                [160, 217, 1],\n",
    "                ...,\n",
    "                [190, 145, 1],\n",
    "                [351, 238, 1]\n",
    "            ],\n",
    "            \"num_keypoints\": 20,\n",
    "            \"category_id\": 1\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"categories\": [\n",
    "        {\n",
    "            \"supercategory\": \"animal\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"dog\",\n",
    "            \"keypoints\": [\n",
    "                \"left_eye\",\n",
    "                \"right_eye\",\n",
    "                ...\n",
    "                \"throat\",\n",
    "                \"withers\",\n",
    "                \"tailbase\"\n",
    "            ],\n",
    "            \"skeleton\": [\n",
    "                [0, 1],\n",
    "                [0, 2],\n",
    "                ...\n",
    "                [11, 15],\n",
    "                [12, 16]\n",
    "            ]\n",
    "        },\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "sdg8Jko7gSAg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train / Val split\n",
    "\n",
    "Before we start training, we have to splint our dataset into training & validation parts. Since official dataset does not provide such split, we will pick the most simple approach here - a stratified split with ratio of 80/20% for train and test respectively. A stratification will be performed based on animal type to ensure we have all species in both splits."
   ],
   "metadata": {
    "id": "e5oLiaBs3688"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_animal_pose_dataset(annotation_file: str, train_annotation_file: str, val_annotation_file: str, val_fraction: float):\n",
    "    with open(annotation_file, \"r\") as f:\n",
    "        annotation = json.load(f)\n",
    "\n",
    "    image_ids = list(annotation[\"images\"].keys())\n",
    "    labels = [[ann[\"category_id\"] for ann in annotation[\"annotations\"] if ann[\"image_id\"] == image_id] for image_id in image_ids]\n",
    "    labels = [label[0] if len(label) else -1 for label in labels]\n",
    "\n",
    "    train_ids, val_ids = train_test_split(image_ids, test_size=val_fraction, random_state=42, stratify=labels)\n",
    "\n",
    "    train_annotations = {\n",
    "        \"info\": annotation[\"info\"],\n",
    "        \"categories\": annotation[\"categories\"],\n",
    "        \"images\": dict((image_id, annotation[\"images\"][image_id]) for image_id in train_ids),\n",
    "        \"annotations\": [ann for ann in annotation[\"annotations\"] if str(ann[\"image_id\"]) in train_ids],\n",
    "    }\n",
    "\n",
    "    val_annotations = {\n",
    "        \"info\": annotation[\"info\"],\n",
    "        \"categories\": annotation[\"categories\"],\n",
    "        \"images\": dict((image_id, annotation[\"images\"][image_id]) for image_id in val_ids),\n",
    "        \"annotations\": [ann for ann in annotation[\"annotations\"] if str(ann[\"image_id\"]) in val_ids],\n",
    "    }\n",
    "\n",
    "    with open(train_annotation_file, \"w\") as f:\n",
    "        json.dump(train_annotations, f)\n",
    "        print(\"Train annotations saved to\", train_annotation_file)\n",
    "        print(\"Train images:\", len(train_ids))\n",
    "        print(\"Train annotations:\", len(train_annotations[\"annotations\"]))\n",
    "\n",
    "    with open(val_annotation_file, \"w\") as f:\n",
    "        json.dump(val_annotations, f)\n",
    "        print(\"Val annotations saved to\", val_annotation_file)\n",
    "        print(\"Val images:\", len(val_ids))\n",
    "        print(\"Val annotations:\", len(val_annotations[\"annotations\"]))\n"
   ],
   "metadata": {
    "id": "wyx-tQgY4WRD"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function `split_animal_pose_dataset` takes full JSON file and split it into two files for train & validation datasets."
   ],
   "metadata": {
    "id": "2NqWQp3R8TXc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "split_animal_pose_dataset(\n",
    "    annotation_file = \"/content/keypoints.json\",\n",
    "    train_annotation_file = \"/content/train_keypoints.json\",\n",
    "    val_annotation_file = \"/content/val_keypoints.json\",\n",
    "    val_fraction = 0.2\n",
    ")"
   ],
   "metadata": {
    "id": "C5jeLAZ08bVH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "283c3eb7-a762-4b2d-e761-ad9eaf745f12"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing a custom dataset class\n",
    "\n",
    "To train pose estimation model using Super Gradients we need to implement a custom dataset class that will parse this format and return images and targets for the model.\n",
    "Fortunately, Super Gradients provides a base class for pose estimation datasets that handles most of the boilerplate code for us.\n",
    "\n",
    "\n",
    "We need to implement a dataset parsing method that will return annotations in a format that Super Gradients expects.\n",
    "The dataset class is expected to have the following method signature:\n",
    "\n",
    "```python\n",
    "class AnimalsPoseDataset:\n",
    "    def __getitem__(self, index):\n",
    "        ...\n",
    "        return image, targets, {\"gt_joints\": gt_joints, \"gt_bboxes\": gt_bboxes, \"gt_iscrowd\": gt_iscrowd, \"gt_areas\": gt_areas}\n",
    "```\n",
    "\n",
    "The `__getitem__` method is expected to return a tuple of 3 elements `image`, `targets` and `extras`:\n",
    "\n",
    "* `image` - torch tensor of [C,H,W] shape that represents an input image to the model.\n",
    "* `targets` - model-specific targets to train the model itself. Fortunately SG will take care of generating these targets for us. Our goal is to provide the keypoints of [Num Instances, Num Joints, 3] shape.\n",
    "* `extras` - Additional information with poses for metric computation. Must be a dictionary with following keys:\n",
    "    * gt_joints - Array of keypoints for all poses in the image. Numpy array of [Num Instances, Num Joints, 3] shape\n",
    "    * gt_bboxes - Array of bounding boxes (XYWH) for each pose in the image. Numpy array of [Num Instances, 4] shape\n",
    "    * gt_iscrowd - Array of iscrowd flags for each pose in the image. Numpy array of [Num Instances] shape\n",
    "    * gt_areas - Array of areas for each skeleton in the image. Numpy array of [Num Instances] shape"
   ],
   "metadata": {
    "id": "MWIrO7SP332N"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Mapping, Any, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "from super_gradients.common.decorators.factory_decorator import resolve_param\n",
    "from super_gradients.common.factories.target_generator_factory import TargetGeneratorsFactory\n",
    "from super_gradients.common.factories.transforms_factory import TransformsFactory\n",
    "from super_gradients.training.datasets.pose_estimation_datasets import BaseKeypointsDataset\n",
    "from super_gradients.training.transforms.keypoint_transforms import KeypointTransform\n",
    "\n",
    "\n",
    "class AnimalPoseKeypointsDataset(BaseKeypointsDataset):\n",
    "    \"\"\"\n",
    "    Dataset class for training pose estimation models on Animal Pose dataset.\n",
    "    User should pass a target generator class that is model-specific and generates the targets for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    @resolve_param(\"transforms\", TransformsFactory())\n",
    "    @resolve_param(\"target_generator\", TargetGeneratorsFactory())\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        images_dir: str,\n",
    "        json_file: str,\n",
    "        include_empty_samples: bool,\n",
    "        target_generator,\n",
    "        transforms: List[KeypointTransform],\n",
    "        min_instance_area: float,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param data_dir: Root directory of the COCO dataset\n",
    "        :param images_dir: path suffix to the images directory inside the data_dir\n",
    "        :param json_file: path suffix to the json file inside the data_dir\n",
    "        :param include_empty_samples: Not used, but exists for compatibility with COCO dataset config.\n",
    "        :param target_generator: Target generator that will be used to generate the targets for the model.\n",
    "            See DEKRTargetsGenerator for an example.\n",
    "        :param transforms: Transforms to be applied to the image & keypoints\n",
    "        :param min_instance_area: Minimum area of an instance to be included in the dataset\n",
    "        \"\"\"\n",
    "        with open(os.path.join(data_dir, json_file), \"r\") as f:\n",
    "            json_annotations = json.load(f)\n",
    "        joints = json_annotations[\"categories\"][0][\"keypoints\"]\n",
    "        skeleton = json_annotations[\"categories\"][0][\"skeleton\"]\n",
    "\n",
    "        num_joints = len(joints)\n",
    "\n",
    "        super().__init__(transforms=transforms, target_generator=target_generator, min_instance_area=min_instance_area, num_joints=num_joints, edge_links=skeleton, edge_colors=None, keypoint_colors=None)\n",
    "\n",
    "\n",
    "        self.joints = joints\n",
    "        self.num_joints = num_joints\n",
    "\n",
    "        images_and_ids = [(image_id, os.path.join(data_dir, images_dir, image_path)) for image_id, image_path in json_annotations[\"images\"].items()]\n",
    "        self.image_ids, self.image_files = zip(*images_and_ids)\n",
    "\n",
    "        self.annotations = []\n",
    "\n",
    "        for image_id in self.image_ids:\n",
    "            keypoints_per_image = []\n",
    "            bboxes_per_image = []\n",
    "\n",
    "            image_annotations = [ann for ann in json_annotations[\"annotations\"] if str(ann[\"image_id\"]) == str(image_id)]\n",
    "            for ann in image_annotations:\n",
    "                keypoints = np.array(ann[\"keypoints\"]).reshape(self.num_joints, 3)\n",
    "                x1, y1, x2, y2 = ann[\"bbox\"]\n",
    "\n",
    "                bbox_xywh = np.array([x1, y1, x2 - x1, y2 - y1])\n",
    "                keypoints_per_image.append(keypoints)\n",
    "                bboxes_per_image.append(bbox_xywh)\n",
    "\n",
    "            keypoints_per_image = np.array(keypoints_per_image, dtype=np.float32).reshape(-1, self.num_joints, 3)\n",
    "            bboxes_per_image = np.array(bboxes_per_image, dtype=np.float32).reshape(-1, 4)\n",
    "            annotation = keypoints_per_image, bboxes_per_image\n",
    "            self.annotations.append(annotation)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def load_sample(self, index):\n",
    "        file_path = self.image_files[index]\n",
    "        gt_joints, gt_bboxes = self.annotations[index]  # boxes in xywh format\n",
    "\n",
    "        gt_areas = np.array([box[2] * box[3] for box in gt_bboxes], dtype=np.float32)\n",
    "        gt_iscrowd = np.array([0] * len(gt_joints), dtype=bool)\n",
    "\n",
    "        image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "        mask = np.ones(image.shape[:2], dtype=np.float32)\n",
    "\n",
    "        return image, mask, gt_joints, gt_areas, gt_bboxes, gt_iscrowd\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Any, Mapping[str, Any]]:\n",
    "        img, mask, gt_joints, gt_areas, gt_bboxes, gt_iscrowd = self.load_sample(index)\n",
    "        img, mask, gt_joints, gt_areas, gt_bboxes = self.transforms(img, mask, gt_joints, areas=gt_areas, bboxes=gt_bboxes)\n",
    "\n",
    "        image_shape = img.size(1), img.size(2)\n",
    "        gt_joints, gt_areas, gt_bboxes, gt_iscrowd = self.filter_joints(image_shape, gt_joints, gt_areas, gt_bboxes, gt_iscrowd)\n",
    "\n",
    "        targets = self.target_generator(img, gt_joints, mask)\n",
    "        return img, targets, {\"gt_joints\": gt_joints, \"gt_bboxes\": gt_bboxes, \"gt_iscrowd\": gt_iscrowd, \"gt_areas\": gt_areas}\n",
    "\n",
    "    def filter_joints(\n",
    "        self,\n",
    "        image_shape,\n",
    "        joints: np.ndarray,\n",
    "        areas: np.ndarray,\n",
    "        bboxes: np.ndarray,\n",
    "        is_crowd: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter instances that are either too small or do not have visible keypoints.\n",
    "\n",
    "        :param image: Image if [H,W,C] shape. Used to infer image boundaries\n",
    "        :param joints: Array of shape [Num Instances, Num Joints, 3]\n",
    "        :param areas: Array of shape [Num Instances] with area of each instance.\n",
    "                      Instance area comes from segmentation mask from COCO annotation file.\n",
    "        :param bboxes: Array of shape [Num Instances, 4] for bounding boxes in XYWH format.\n",
    "                       Bounding boxes comes from segmentation mask from COCO annotation file.\n",
    "        :param: is_crowd: Array of shape [Num Instances] indicating whether an instance is a crowd target.\n",
    "        :return: [New Num Instances, Num Joints, 3], New Num Instances <= Num Instances\n",
    "        \"\"\"\n",
    "\n",
    "        # Update visibility of joints for those that are outside the image\n",
    "        outside_image_mask = (joints[:, :, 0] < 0) | (joints[:, :, 1] < 0) | (joints[:, :, 0] >= image_shape[1]) | (joints[:, :, 1] >= image_shape[0])\n",
    "        joints[outside_image_mask, 2] = 0\n",
    "\n",
    "        # Filter instances with all invisible keypoints\n",
    "        instances_with_visible_joints = np.count_nonzero(joints[:, :, 2], axis=-1) > 0\n",
    "        instances_with_good_area = areas > self.min_instance_area\n",
    "\n",
    "        keep_mask = instances_with_visible_joints & instances_with_good_area\n",
    "\n",
    "        joints = joints[keep_mask]\n",
    "        areas = areas[keep_mask]\n",
    "        bboxes = bboxes[keep_mask]\n",
    "        is_crowd = is_crowd[keep_mask]\n",
    "\n",
    "        return joints, areas, bboxes, is_crowd\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:18.059324400Z",
     "start_time": "2023-06-05T11:08:10.206196400Z"
    },
    "id": "8hQFvXHZgSAh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7fb8e204-ed77-46ef-cf43-e9a2f637821f"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's recap what are the responsibilities of `AnimalPoseKeypointsDataset` class:\n",
    "\n",
    "- Load annotations from JSON file in COCO format\n",
    "- Unpack annotations into keypoints and bounding boxes\n",
    "- Implements a `__getitem__` method for retrieving individual image with corresponding annotations\n",
    "- Apply preprocessing and data augmentation transforms to the image and annotations\n",
    "- Encodes a \"human friendly\" joints representation (Array of `[Num Instances, Num Joints, 3]` shape) into a format suitable for training Yolo-NAS (Tuple of two tensors `[NumJoints + 1, H/4, W/4]`, `[NumJoints*2, H/4, W/4]`).\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "412opkkwgSAi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, when we have the dataset class we can instantiate DataLoaders.\n",
    "Since our fine-tuning tasks is really similar to COCO human pose estimation task we can take COCO dataset configs as base ones and use transforms & target generator from these configs:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "SJYGVyIQgSAi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from typing import Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from super_gradients.training.dataloaders import get_data_loader\n",
    "\n",
    "def animalpose_pose_train(dataset_params: Dict = None, dataloader_params: Dict = None) -> DataLoader:\n",
    "    return get_data_loader(\n",
    "        config_name=\"coco_pose_estimation_dekr_dataset_params\",\n",
    "        dataset_cls=AnimalPoseKeypointsDataset,\n",
    "        train=True,\n",
    "        dataset_params=dataset_params,\n",
    "        dataloader_params=dataloader_params,\n",
    "    )\n",
    "\n",
    "\n",
    "def animalpose_pose_val(dataset_params: Dict = None, dataloader_params: Dict = None) -> DataLoader:\n",
    "    return get_data_loader(\n",
    "        config_name=\"coco_pose_estimation_dekr_dataset_params\",\n",
    "        dataset_cls=AnimalPoseKeypointsDataset,\n",
    "        train=False,\n",
    "        dataset_params=dataset_params,\n",
    "        dataloader_params=dataloader_params,\n",
    "    )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:18.074328300Z",
     "start_time": "2023-06-05T11:08:18.060326500Z"
    },
    "id": "0X0NJaKogSAj"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are almost ready to instantiate our data loaders. There is only one important nuance that we have to cover. Let's instantiate our train dataset and inspect the transformations that we apply to our samples:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Eya_ltRogSAj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from pprint import pprint\n",
    "\n",
    "train_data = animalpose_pose_train(\n",
    "    dataset_params=dict(data_dir=ANIMALS_POSE_DATA_DIR, images_dir=\"images\", json_file=\"train_keypoints.json\"),\n",
    "    dataloader_params=dict(num_workers=0, batch_size=8)\n",
    ")\n",
    "train_data.dataset.transforms.transforms"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:28.946373600Z",
     "start_time": "2023-06-05T11:08:18.078325100Z"
    },
    "id": "73spfmakgSAj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8663f107-4906-4c4b-92a0-bce189a4558f"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "During training we apply several augmentations to our samples. There is one, however that needs out special attention, which is KeypointsRandomHorizontalFlip.\n",
    "When working with object detection and semantic segmentation we can safely skip the fact that left and right in flipped image changing sides.\n",
    "However, in pose estimation of objects that possess vertical symmetry, like animals or humans, we have to take this into account.\n",
    "So when we flip the image we also have to swap left and right keypoints. In order to do this correctly KeypointsRandomHorizontalFlip transform must know the rearrange indices for keypoints.\n",
    "\n",
    "If we look at the JSON data of your annotation file, we can see that the order of keypoints is the following:\n",
    "```\n",
    "# 0 \"left_eye\" -> \"right_eye\": 1\n",
    "# 1 \"right_eye\" -> \"left_eye\": 0\n",
    "# 2 \"nose\" -> \"nose\": 2\n",
    "# 3 \"left_ear\" -> \"right_ear\": 4\n",
    "# 4 \"right_ear\" -> \"left_ear\": 3\n",
    "# 5 \"left_front_elbow\" -> \"right_front_elbow\": 6\n",
    "# 6 \"right_front_elbow\" -> \"left_front_elbow\": 5\n",
    "# 7 \"left_back_elbow\" -> \"right_back_elbow\": 8\n",
    "# 8 \"right_back_elbow\" -> \"left_back_elbow\": 7\n",
    "# 9 \"left_front_knee\" -> \"right_front_knee\": 10\n",
    "# 10 \"right_front_knee\" -> \"left_front_knee\": 9\n",
    "# 11 \"left_back_knee\" -> \"right_back_knee\": 12\n",
    "# 12 \"right_back_knee\" -> \"left_back_knee\": 11\n",
    "# 13 \"left_front_paw\" -> \"right_front_paw\": 14\n",
    "# 14 \"right_front_paw\" -> \"left_front_paw\": 13\n",
    "# 15 \"left_back_paw\" -> \"right_back_paw\": 16\n",
    "# 16 \"right_back_paw\" -> \"left_back_paw\": 15\n",
    "# 17 \"throat\" -> \"throat\": 17\n",
    "# 18 \"withers\" -> \"withers\": 18\n",
    "# 10 \"tailbase\" -> \"tailbase\": 19\n",
    "```\n",
    "\n",
    "So our array of indexes will look like this:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "QFJridLggSAj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "ANIMALS_POSE_FLIP_INDEXES = [1,0,2,4,3,6,5,8,7,10,9,12,11,14,13,16,15,17,18,19]"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:28.991178700Z",
     "start_time": "2023-06-05T11:08:28.950374800Z"
    },
    "id": "mIJEmtjGgSAk"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For training data loader we have to pass the transforms explicitly in order to override the default ones with the right flip transform:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "vSnpRcoDgSAk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from super_gradients.training.transforms import (\n",
    "    KeypointsLongestMaxSize,\n",
    "    KeypointsPadIfNeeded,\n",
    "    KeypointsRandomHorizontalFlip,\n",
    "    KeypointsRandomAffineTransform,\n",
    "    KeypointsImageNormalize,\n",
    "    KeypointsImageToTensor,\n",
    "    KeypointsImageStandardize\n",
    ")\n",
    "\n",
    "train_transforms = [\n",
    "    KeypointsLongestMaxSize(640, 640),\n",
    "    KeypointsPadIfNeeded(640, 640, image_pad_value=127, mask_pad_value=1),\n",
    "    KeypointsRandomHorizontalFlip(ANIMALS_POSE_FLIP_INDEXES),\n",
    "    KeypointsRandomAffineTransform(max_rotation=30, min_scale=0.75, max_scale=1.25, max_translate=0.2, image_pad_value=127, mask_pad_value=1, prob=0.5),\n",
    "    KeypointsImageStandardize(max_value=255),\n",
    "    KeypointsImageNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    KeypointsImageToTensor(),\n",
    "]\n",
    "\n",
    "train_data = animalpose_pose_train(\n",
    "    dataset_params=dict(data_dir=ANIMALS_POSE_DATA_DIR, images_dir=\"images\", json_file=\"train_keypoints.json\", transforms=train_transforms),\n",
    "    dataloader_params=dict(num_workers=4, batch_size=8)\n",
    ")\n",
    "\n",
    "val_data = animalpose_pose_val(\n",
    "    dataset_params=dict(data_dir=ANIMALS_POSE_DATA_DIR, images_dir=\"images\", json_file=\"val_keypoints.json\"),\n",
    "    dataloader_params=dict(num_workers=4, batch_size=8)\n",
    ")\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:51.564385300Z",
     "start_time": "2023-06-05T11:08:28.969004600Z"
    },
    "id": "u5Cdyp7ogSAk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "28f77651-8a81-471c-d450-2202de3ac91e"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train_data.dataset.transforms"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:51.580278800Z",
     "start_time": "2023-06-05T11:08:51.564385300Z"
    },
    "id": "MysWYr-8gSAk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e3c42079-e683-427d-9b56-0e52450e45c0"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "val_data.dataset.transforms"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:51.670651Z",
     "start_time": "2023-06-05T11:08:51.582278400Z"
    },
    "id": "WbOBjyvhgSAk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e92a1d18-bfef-434c-f91a-f65041906e63"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instantiating the model for fine-tuning\n",
    "\n",
    "In this tutorial we will use pre-trained DEKR for pose estimation model. It is trained on COCO dataset for human pose estimation.\n",
    "So if we pass image with animal it will produce a pose estimation predictions for a human:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "GbavvYIrgSAl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "\n",
    "model = models.get(Models.DEKR_W32_NO_DC, pretrained_weights=\"coco_pose\")\n",
    "model.predict(\"/content/images/2007_000392.jpg\").show()"
   ],
   "metadata": {
    "id": "YskxBofMIu1k",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "outputId": "27cdf610-f8e3-443c-8b32-2acdbe67c08e"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Animal Pose dataset the number of joints is set to 20. So we have to change the architecture of the model to output 20 keypoints instead of 17.\n",
    "\n",
    "Super Gradients provide convenient way to instantiate pre-trained models and override the number of output classes: `models.get(Models.DEKR_W32_NO_DC, num_classes=20, pretrained_weights=\"coco\")`.\n",
    "\n",
    "What is happening under the hood in this call is the following:\n",
    "1. `models.get(Models.DEKR_W32_NO_DC, ...)` - this call instantiates the model Models.DEKR_W32_NO_DC with default parameters.\n",
    "2. `models.get(..., num_classes=20, ...)` - this parameter sets the target number of output classes the model should have. Here it is set to 20 as this is the target number of joints in our dataset.\n",
    "3. `models.get(..., pretrained_weights=\"coco\")` - this parameter specify what pretrained weights should be loaded into the model. Here we specify that we want to load weights trained on COCO dataset for human pose estimation.\n"
   ],
   "metadata": {
    "id": "MKgi3H-sJaQH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model = models.get(Models.DEKR_W32_NO_DC, num_classes=20, pretrained_weights=\"coco_pose\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:09:17.261956600Z",
     "start_time": "2023-06-05T11:08:51.611961500Z"
    },
    "id": "NZO6A9I4gSAl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7db24fed-685a-41d0-96a8-c45b33c7ca68"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Training hyperparameters\n",
    "\n",
    "Training DEKR model requires passing a specific loss, which we set up in our training hyperparameters. Vanilla DEKR implementation from the paper use MSE loss for heatmap regression. In our approach we use QFL loss instead. Offset loss remains unchanged and default 'l1' term is used:\n",
    "\n",
    " ```yaml\n",
    "    \"loss\": \"dekr_loss\",\n",
    "    \"criterion_params\": {\n",
    "        \"heatmap_loss\": \"qfl\",\n",
    "        \"heatmap_loss_factor\": 1.0,\n",
    "        \"offset_loss_factor\": 0.1,\n",
    "    }\n",
    "```\n",
    "\n",
    "We do not use warmup for this example, but you can see how to set it up in the commented out section below:\n",
    "\n",
    "```yaml\n",
    "    \"warmup_mode\": \"linear_epoch_step\",\n",
    "    \"warmup_initial_lr\": 1e-3,\n",
    "    \"lr_warmup_epochs\": 0,\n",
    "```\n",
    "\n",
    "To compute the target metric we add `PoseEstimationMetrics` object to `valid_metrics_list`:\n",
    "\n",
    "```yaml\n",
    "    \"valid_metrics_list\": [\n",
    "        PoseEstimationMetrics(\n",
    "            num_joints=20,\n",
    "            oks_sigmas=None,\n",
    "            max_objects_per_image=30,\n",
    "            post_prediction_callback=model.get_post_prediction_callback(conf=0.05, iou=0.05),\n",
    "        )\n",
    "    ],\n",
    "    \"metric_to_watch\": 'AP',\n",
    "```\n",
    "\n",
    "And last but not least, let's add visualization callbacks to see how our training is going. The results will be available in Tensorboard logs:\n",
    "\n",
    "```yaml\n",
    "    \"phase_callbacks\": [\n",
    "        DEKRVisualizationCallback(mean=[ 0.485, 0.456, 0.406 ], std=[ 0.229, 0.224, 0.225 ], apply_sigmoid=True, phase=Phase.TRAIN_BATCH_END, prefix=\"train_\"),\n",
    "        DEKRVisualizationCallback(mean=[ 0.485, 0.456, 0.406 ], std=[ 0.229, 0.224, 0.225 ], apply_sigmoid=True, phase=Phase.VALIDATION_BATCH_END, prefix=\"val\"),\n",
    "    ]\n",
    "```\n",
    "\n",
    "Final training params config should look like this:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6uUtDBBRgSAl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from super_gradients.training.utils import DEKRVisualizationCallback\n",
    "from super_gradients.training.metrics import PoseEstimationMetrics\n",
    "from super_gradients.training.utils.callbacks.callbacks import Phase\n",
    "\n",
    "train_params = {\n",
    "    # \"resume\": True,\n",
    "    \"average_best_models\": True,\n",
    "    \"warmup_mode\": \"linear_epoch_step\",\n",
    "    \"warmup_initial_lr\": 1e-3,\n",
    "    \"lr_warmup_epochs\": 0,\n",
    "    \"initial_lr\": 1e-3,\n",
    "    \"lr_mode\": \"cosine\",\n",
    "    \"cosine_final_lr_ratio\": 0.1,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"optimizer_params\": {\"weight_decay\": 1e-6},\n",
    "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
    "    \"ema\": False,\n",
    "    \"ema_params\": {\"decay\": 0.9, \"decay_type\": \"threshold\"},\n",
    "    # ONLY TRAINING FOR 1 EPOCHS FOR THIS EXAMPLE NOTEBOOK TO REDUCE TRAINING TIME!\n",
    "    # Increase the number of epoch to improve the training results.\n",
    "    \"max_epochs\": 1,\n",
    "    \"mixed_precision\": True,\n",
    "    \"loss\": \"dekr_loss\",\n",
    "    \"criterion_params\": {\n",
    "        \"heatmap_loss\": \"qfl\",\n",
    "        \"heatmap_loss_factor\": 1.0,\n",
    "        \"offset_loss_factor\": 0.1,\n",
    "    },\n",
    "    \"valid_metrics_list\": [\n",
    "        PoseEstimationMetrics(\n",
    "            num_joints=20,\n",
    "            oks_sigmas=None,\n",
    "            max_objects_per_image=30,\n",
    "            post_prediction_callback=model.get_post_prediction_callback(conf=0.05),\n",
    "        )\n",
    "    ],\n",
    "    \"metric_to_watch\": 'AP',\n",
    "    \"phase_callbacks\": [\n",
    "        DEKRVisualizationCallback(mean=[ 0.485, 0.456, 0.406 ], std=[ 0.229, 0.224, 0.225 ], apply_sigmoid=True, phase=Phase.TRAIN_BATCH_END, prefix=\"train_\"),\n",
    "        DEKRVisualizationCallback(mean=[ 0.485, 0.456, 0.406 ], std=[ 0.229, 0.224, 0.225 ], apply_sigmoid=True, phase=Phase.VALIDATION_BATCH_END, prefix=\"val\"),\n",
    "    ]\n",
    "}\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:25:36.323186400Z",
     "start_time": "2023-06-05T11:25:36.289757700Z"
    },
    "id": "pt0mkw_NgSAl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0c652827-42b1-44ce-a24b-56aaa804bcc3"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from super_gradients.training import Trainer\n",
    "\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:25:40.848162Z",
     "start_time": "2023-06-05T11:25:40.815163400Z"
    },
    "id": "KdbuW26qgSAl"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can start a tensorboard with the following code:"
   ],
   "metadata": {
    "id": "V4pFhl_XlzgH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Uncomment this if you want to start the tensorboard\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir checkpoints"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:25:41.409106300Z",
     "start_time": "2023-06-05T11:25:41.366238300Z"
    },
    "id": "JMbDFzgXgSAm"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "trainer = Trainer(experiment_name='animal_pose_fine_tuning', ckpt_root_dir=CHECKPOINT_DIR)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T11:25:49.075229300Z",
     "start_time": "2023-06-05T11:25:49.040542300Z"
    },
    "id": "9xS9FvydgSAm"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "trainer.train(model=model,\n",
    "              training_params=train_params,\n",
    "              train_loader=train_data,\n",
    "              valid_loader=val_data)"
   ],
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-05T11:25:49.340357100Z"
    },
    "id": "ZEM_P2QjgSAm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a106a21b-f31c-42b6-9782-6218f664de04"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting with trained model\n",
    "\n",
    "Let's inspect how our trained model works after training for 1 epochs. For this we use the [predict API](https://docs.deci.ai/super-gradients/documentation/source/ModelPredictions.html) available for Super Gradient models.\n",
    "\n",
    "In the nutshell, all you need to do is to call `model.predict(image)`, where image can be anything from numpy array, image url, a folder with images of video file.  "
   ],
   "metadata": {
    "id": "M2XyFxdNwoXD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = models.get(Models.DEKR_W32_NO_DC, num_classes=20, checkpoint_path=f\"{trainer.checkpoints_dir_path}/ckpt_best.pth\")"
   ],
   "metadata": {
    "id": "F0R6yJind2u9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d883b342-644e-41a2-9db0-0a4057e97525"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.predict(\"https://github.com/Deci-AI/super-gradients/blob/master/documentation/source/images/examples/women.jpeg?raw=true\").show()"
   ],
   "metadata": {
    "id": "Np9Op23YdrMY",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "outputId": "4841012b-77eb-47c3-8195-30f937eac156"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see 1 epoch is not enough for the model to properly learn, but fully training the model would be too slow for this tutorial.\n",
    "\n",
    "Feel free to increase the number of epoch and to tune the hyperparameters."
   ],
   "metadata": {
    "id": "B2oLBZoSdnKr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "To summarize, this notebooks demonstrated:\n",
    "* How to parse external dataset and implement a custom dataset class for pose estimation task\n",
    "* How to leverate existing base class for pose estimation tasks from Super Gradients\n",
    "* How to override dataset transforms to ensure transforms operate on a correct skeleton definition\n",
    "* How to instantiate DEKR model and override number of output joints for animal pose estimation\n",
    "* How to train model and visualize training progress using Tensorboard\n",
    "* How to use predict API to visualize the results\n",
    "\n",
    "## Home assignment\n",
    "\n",
    "We hope this tutorial inspire you to play with different hyperparameters and maybe other datasets. Here are a few things you can try and see what happens:\n",
    "\n",
    "1. Try increasing number of training epochs. 10 epochs can be too few\n",
    "2. You can also try training a smaller model, since it may train in less time.\n",
    "3. Try different learning rate, EMA settings and weight decay\n",
    "4. Investigate what the impact on assigning different weights to heatmap & offset components of the loss.\n",
    "\n",
    "We wish you good luck on your journey. If there is any questions or issues with the tutorial, feel free to reach out via [Super Gradient's Issues](https://github.com/Deci-AI/super-gradients/issues) page."
   ],
   "metadata": {
    "id": "g1w0osbq08j2"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
